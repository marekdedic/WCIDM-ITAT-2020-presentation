
@inproceedings{muandet_learning_2012,
	title = {Learning from {Distributions} via {Support} {Measure} {Machines}},
	url = {http://papers.nips.cc/paper/4825-learning-from-distributions-via-support-measure-machines},
	urldate = {2017-06-29},
	booktitle = {Advances in neural information processing systems},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Dinuzzo, Francesco and Schölkopf, Bernhard},
	year = {2012},
	pages = {10--18}
}

@article{dietterich_solving_1997,
	title = {Solving the multiple instance problem with axis-parallel rectangles},
	volume = {89},
	issn = {0004-3702},
	doi = {10.1016/S0004-3702(96)00034-3},
	abstract = {The multiple instance problem arises in tasks where the training examples are ambiguous: a single example object may have many alternative feature vectors (instances) that describe it, and yet only one of those feature vectors may be responsible for the observed classification of the object. This paper describes and compares three kinds of algorithms that learn axis-parallel rectangles to solve the multiple instance problem. Algorithms that ignore the multiple instance problem perform very poorly. An algorithm that directly confronts the multiple instance problem (by attempting to identify which feature vectors are responsible for the observed classifications) performs best, giving 89\% correct predictions on a musk odor prediction task. The paper also illustrates the use of artificial data to debug and compare these algorithms.},
	number = {1},
	urldate = {2017-05-31},
	journal = {Artificial Intelligence},
	author = {Dietterich, Thomas G. and Lathrop, Richard H. and Lozano-Pérez, Tomás},
	month = jan,
	year = {1997},
	keywords = {Drug design, Machine learning, Structure-activity relationships},
	pages = {31--71}
}

@inproceedings{zelnik-manor_self-tuning_2005,
	title = {Self-{Tuning} {Spectral} {Clustering}},
	url = {http://papers.nips.cc/paper/2619-self-tuning-spectral-clustering.pdf},
	urldate = {2020-01-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 17},
	publisher = {MIT Press},
	author = {Zelnik-manor, Lihi and Perona, Pietro},
	editor = {Saul, L. K. and Weiss, Y. and Bottou, L.},
	year = {2005},
	pages = {1601--1608},
	file = {NIPS Snapshot:/home/cisco/Zotero/storage/G53QHJUY/2619-self-tuning-spectral-clustering.html:text/html;NIPS Full Text PDF:/home/cisco/Zotero/storage/QXSXUMLF/Zelnik-manor and Perona - 2005 - Self-Tuning Spectral Clustering.pdf:application/pdf}
}

@inproceedings{briggs_rank-loss_2012,
	title = {Rank-loss support instance machines for {MIML} instance annotation},
	doi = {10.1145/2339530.2339616},
	abstract = {Multi-instance multi-label learning (MIML) is a framework for supervised classification where the objects to be classified are bags of instances associated with multiple labels. For example, an image can be represented as a bag of segments and associated with a list of objects it contains. Prior work on MIML has focused on predicting label sets for previously unseen bags. We instead consider the problem of predicting instance labels while learning from data labeled only at the bag level. We propose Rank-Loss Support Instance Machines, which optimize a regularized rank-loss objective and can be instantiated with different aggregation models connecting instance-level predictions with bag-level predictions. The aggregation models that we consider are equivalent to defining a "support instance" for each bag, which allows efficient optimization of the rank-loss objective using primal sub-gradient descent. Experiments on artificial and real-world datasets show that the proposed methods achieve higher accuracy than other loss functions used in prior work, e.g., Hamming loss, and recent work in ambiguous label classification.},
	booktitle = {Proceedings of the {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Briggs, Forrest and Fern, Xiaoli and Raich, Raviv},
	month = aug,
	year = {2012},
	file = {Full Text PDF:/home/cisco/Zotero/storage/XM4XPD5R/Briggs et al. - 2012 - Rank-loss support instance machines for MIML insta.pdf:application/pdf}
}

@inproceedings{ray_supervised_2005,
	title = {Supervised versus multiple instance learning: {An} empirical comparison},
	shorttitle = {Supervised versus multiple instance learning},
	doi = {10.1145/1102351.1102439},
	abstract = {We empirically study the relationship be- tween supervised and multiple instance (MI) learning. Algorithms to learn various con- cepts have been adapted to the MI represen- tation. However, it is also known that con- cepts that are PAC-learnable with one-sided noise can be learned from MI data. A rel- evant question then is: how well do super- vised learners do on MI data? We attempt to answer this question by looking at a cross section of MI data sets from various domains coupled with a number of learning algorithms including Diverse Density, Logistic Regres- sion, nonlinear Support Vector Machines and FOIL. We consider a supervised and MI ver- sion of each learner. Several interesting con- clusions emerge from our work: (1) no MI al- gorithm is superior across all tested domains, (2) some MI algorithms are consistently su- perior to their supervised counterparts, (3) using high false-positive costs can improve a supervised learner's performance in MI do- mains, and (4) in several domains, a super- vised algorithm is superior to any MI algo- rithm we tested.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Machine} {Learning}},
	author = {Ray, Soumya and Craven, Mark},
	month = jan,
	year = {2005},
	pages = {697--704}
}

@article{andrews_support_2002,
	title = {Support {Vector} {Machines} for {Multiple}-{Instance} {Learning}},
	volume = {15},
	abstract = {This paper presents two new formulations of multiple-instance learning as a maximum margin problem. The proposed extensions of the Support Vector Machine (SVM) learning approach lead to mixed integer quadratic programs that can be solved heuristically. Our generalization of SVMs makes a state-of-the-art classication technique, including non-linear classication via kernels, available to an area that up to now has been largely dominated by special purpose methods. We present experimental results on a pharma- ceutical data set and on applications in automated image indexing and document categorization.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Andrews, Stuart and Tsochantaridis, Ioannis and Hofmann, Thomas},
	month = jan,
	year = {2002},
	pages = {561--568},
	file = {Full Text PDF:/home/cisco/Zotero/storage/6H8Z43A4/Andrews et al. - 2002 - Support Vector Machines for Multiple-Instance Lear.pdf:application/pdf}
}

@article{ray_learning_2005,
	title = {Learning {Statistical} {Models} for {Annotating} {Proteins} with {Function} {Information} using {Biomedical} {Text}},
	volume = {6},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/1471-2105-6-S1-S18},
	doi = {10.1186/1471-2105-6-S1-S18},
	abstract = {The BioCreative text mining evaluation investigated the application of text mining methods to the task of automatically extracting information from text in biomedical research articles. We participated in Task 2 of the evaluation. For this task, we built a system to automatically annotate a given protein with codes from the Gene Ontology (GO) using the text of an article from the biomedical literature as evidence.},
	number = {1},
	urldate = {2020-01-01},
	journal = {BMC Bioinformatics},
	author = {Ray, Soumya and Craven, Mark},
	month = may,
	year = {2005},
	pages = {S18},
	file = {Full Text:/home/cisco/Zotero/storage/FUPK8UEP/Ray and Craven - 2005 - Learning Statistical Models for Annotating Protein.pdf:application/pdf;Snapshot:/home/cisco/Zotero/storage/VL88HZLW/1471-2105-6-S1-S18.html:text/html}
}

@inproceedings{srinivasan_comparing_1995,
	title = {Comparing the use of background knowledge by inductive logic programming systems},
	booktitle = {Proceedings of the 5th {International} {Workshop} on {Inductive} {Logic} {Programming}},
	author = {Srinivasan, A. and Muggleton, Stephen and King, Robert},
	year = {1995},
	keywords = {Inductive logic programming},
	pages = {199--230}
}

@article{zhou_multi-instance_2008,
	title = {Multi-{Instance} {Learning} by {Treating} {Instances} {As} {Non}-{I}.{I}.{D}. {Samples}},
	doi = {10.1145/1553374.1553534},
	abstract = {Multi-instance learning attempts to learn from a training set consisting of labeled bags each containing many unlabeled instances. Previous studies typically treat the instances in the bags as independently and identically distributed. However, the instances in a bag are rarely independent, and therefore a better performance can be expected if the instances are treated in an non-i.i.d. way that exploits the relations among instances. In this paper, we propose a simple yet effective multi-instance learning method, which regards each bag as a graph and uses a specific kernel to distinguish the graphs by considering the features of the nodes as well as the features of the edges that convey some relations among instances. The effectiveness of the proposed method is validated by experiments. Comment: ICML, 2009},
	journal = {Proceedings of the 26th International Conference On Machine Learning, ICML 2009},
	author = {Zhou, Zhi-Hua and Sun, Yu-Yin and Li, Yu-Feng},
	month = jul,
	year = {2008}
}

@article{zhou_multi-instance_2005,
	title = {Multi-{Instance} {Learning} {Based} {Web} {Mining}},
	volume = {22},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-005-5602-z},
	doi = {10.1007/s10489-005-5602-z},
	abstract = {In multi-instance learning, the training set comprises labeled bags that are composed of unlabeled instances, and the task is to predict the labels of unseen bags. In this paper, a web mining problem, i.e. web index recommendation, is investigated from a multi-instance view. In detail, each web index page is regarded as a bag, while each of its linked pages is regarded as an instance. A user favoring an index page means that he or she is interested in at least one page linked by the index. Based on the browsing history of the user, recommendation could be provided for unseen index pages. An algorithm named Fretcit-kNN, which employs the Minimal Hausdorff distance between frequent term sets and utilizes both the references and citers of an unseen bag in determining its label, is proposed to solve the problem. Experiments show that in average the recommendation accuracy of Fretcit-kNN is 81.0\% with 71.7\% recall and 70.9\% precision, which is significantly better than the best algorithm that does not consider the specific characteristics of multi-instance learning, whose performance is 76.3\% accuracy with 63.4\% recall and 66.1\% precision.},
	number = {2},
	urldate = {2020-01-01},
	journal = {Applied Intelligence},
	author = {Zhou, Zhi-Hua and Jiang, Kai and Li, Ming},
	month = mar,
	year = {2005},
	keywords = {data mining, machine learning, multi-instance learning, text categorization, web index recommendation, web mining},
	pages = {135--147}
}

@inproceedings{kandemir_empowering_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Empowering {Multiple} {Instance} {Histopathology} {Cancer} {Diagnosis} by {Cell} {Graphs}},
	isbn = {978-3-319-10470-6},
	doi = {10.1007/978-3-319-10470-6_29},
	abstract = {We introduce a probabilistic classifier that combines multiple instance learning and relational learning. While multiple instance learning allows automated cancer diagnosis from only image-level annotations, relational learning allows exploiting changes in cell formations due to cancer. Our method extends Gaussian process multiple instance learning with a relational likelihood that brings improved diagnostic performance on two tissue microarray data sets (breast and Barrett’s cancer) when similarity of cell layouts in different tissue regions is used as relational side information.},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2014},
	publisher = {Springer International Publishing},
	author = {Kandemir, Melih and Zhang, Chong and Hamprecht, Fred A.},
	editor = {Golland, Polina and Hata, Nobuhiko and Barillot, Christian and Hornegger, Joachim and Howe, Robert},
	year = {2014},
	keywords = {Local Binary Pattern, Marginal Likelihood, Multiple Instance, Receiver Operating Characteristic Curve, Side Information},
	pages = {228--235},
	file = {Springer Full Text PDF:/home/cisco/Zotero/storage/Q7PDTMBN/Kandemir et al. - 2014 - Empowering Multiple Instance Histopathology Cancer.pdf:application/pdf}
}

@techreport{arthur_k-means++:_2006,
	type = {Technical {Report}},
	title = {k-means++: {The} {Advantages} of {Careful} {Seeding}},
	shorttitle = {k-means++},
	url = {http://ilpubs.stanford.edu:8090/778/},
	abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a simple, randomized seeding technique, we obtain an algorithm that is \$O({\textbackslash}log k)\$-competitive with the optimal clustering. Experiments show our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
	number = {2006-13},
	institution = {Stanford InfoLab},
	author = {Arthur, David and Vassilvitskii, Sergei},
	month = jun,
	year = {2006},
	keywords = {clustering, k-means, seeding},
	pages = {1027--1035}
}

@inproceedings{macqueen_methods_1967,
	address = {Berkeley, California},
	title = {Some methods for classification and analysis of multivariate observations},
	volume = {1},
	abstract = {The main purpose of this paper is to describe a process for partitioning an N-dimensional population into k sets on the basis of a sample. The process, which is called 'k-means, ' appears to give partitions which are reasonably},
	booktitle = {Proceedings of the {Fifth} {Berkeley} {Symposium} on {Mathematical} {Statistics} and {Probability}},
	publisher = {University of California Press},
	author = {MacQueen, J.},
	year = {1967},
	pages = {281--297},
	file = {Citeseer - Full Text PDF:/home/cisco/Zotero/storage/774Y45YA/Macqueen - 1967 - Some methods for classification and analysis of mu.pdf:application/pdf;Citeseer - Snapshot:/home/cisco/Zotero/storage/EIUCNY4H/summary.html:text/html}
}

@article{elias_predictive_1955,
	title = {Predictive coding–{I}},
	volume = {1},
	issn = {2168-2712},
	doi = {10.1109/TIT.1955.1055126},
	abstract = {Predictive coding is a procedure for transmitting messages which are sequences of magnitudes. In this coding method, the transmitter and the receiver store past message terms, and from them estimate the value of the next message term. The transmitter transmits, not the message term, but the difference between it and its predicted value. At the receiver this error term is added to the receiver prediction to reproduce the message term. This procedure is defined and messages, prediction, entropy, and ideal coding are discussed to provide a basis for Part II, which will give the mathematical criterion for the best predictor for use in the predictive coding of particular messages, will give examples of such messages, and will show that the error term which is transmitted in predictive coding may always be coded efficiently.},
	number = {1},
	journal = {IRE Transactions on Information Theory},
	author = {Elias, P.},
	month = mar,
	year = {1955},
	keywords = {Bandwidth, Decoding, Entropy, Error correction codes, Feeds, Information filtering, Information filters, Predictive coding, Pulse modulation, Transmitters},
	pages = {16--24},
	file = {IEEE Xplore Abstract Record:/home/cisco/Zotero/storage/VXR75JIQ/1055126.html:text/html}
}

@article{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2019-12-29},
	journal = {arXiv:1807.03748 [cs, stat]},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv: 1807.03748},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/cisco/Zotero/storage/Y6VZYCEP/1807.html:text/html;arXiv Fulltext PDF:/home/cisco/Zotero/storage/NURFJCQR/Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf:application/pdf}
}

@book{everitt_cluster_2001,
	edition = {4},
	title = {Cluster {Analysis}},
	isbn = {978-0-340-76119-9},
	abstract = {Cluster analysis comprises a range of methods for classifying multivariate data into subgroups. By organising multivariate data into such subgroups, clustering can help reveal the characteristics of any structure or patterns present. These techniques are applicable in a wide range of areas such as medicine, psychology and market research. This fourth edition of the highly successful Cluster Analysis represents a thorough revision of the third edition and covers new and developing areas such as classification likelihood and neural networks for clustering. Real life examples are used throughout to demonstrate the application of the theory, and figures are used extensively to illustrate graphical techniques. The book is comprehensive yet relatively non-mathematical, focusing on the practical aspects of cluster analysis.},
	publisher = {Taylor \& Francis},
	author = {Everitt, Brian S. and Landau, Sabine and Leese, Morven},
	year = {2001},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Multivariate Analysis, Mathematics / Probability \& Statistics / Stochastic Processes}
}

@phdthesis{jusko_graph-based_2017,
	address = {Prague},
	title = {Graph-based {Detection} of {Malicious} {Network} {Communities}},
	url = {https://dspace.cvut.cz/handle/10467/73702},
	abstract = {In this thesis, we use graph based methods in conjunction with behavioral modeling to uncover 
hidden malicious communities and peer-to-peer tra c. 
The nature of malicious tra c, and its tendency to rally in order to communicate with 
its owner opens a possibility to detect malicious tra c by revealing hidden sub-structures of 
network tra c. In fact, besides discovering the presence of an infection, analyzing network 
tra c also enables inference of valuable context information about the malicious campaign 
as a whole, often leading to a more precise attribution than is possible using only a hostbased 
solution. In this work, we focus on the detection approaches that observe the hidden 
structures and exploit them to uncover malicious command \& control (C\&C) servers. 
Peer-to-peer (P2P) protocol is a popular choice with malware authors to be used as a C\&C 
channel. Therefore, we propose a uni ed solution to identify P2P communities operating in a 
monitored network. We propose an algorithm that is able to 1) progressively discover hosts in 
the monitored network that cooperate in a P2P network and to 2) identify that P2P network. 
Starting from a single known host, other hosts participating in the P2P network are identi ed 
through the analysis of widely available and standardized IPFIX (NetFlow) data. It is able 
to identify a large range of both legitimate and malicious P2P networks, is highly scalable 
and the use of standard meta-data without access to tra c content makes it easy to deploy 
and justify from privacy protection perspective. 
Even malware families that do not rely on a P2P-based C\&C channels resort to highly 
dynamic C\&C structures to counter security industry approaches based on blacklisting known 
malicious domains. It is therefore important to automatically follow the migration of C\&C 
servers. We propose to use a well-known Probability Threat Propagation (PTP) with a novel 
graph representation capturing connections from clients to servers. The proposed graph 
representation is highly condensed, preserves privacy, allows us to  nd malicious domains 
that cannot be found using existing graph representations and is harder to evade by malware 
authors. 
We propose two behavioral models for HTTP tra c together with kernel-based similarity 
and distance functions that can be conveniently used to extend the  ndings of PTP. For 
any domain marked as malicious by PTP we can  nd other domains with identical or similar 
behavior, which are likely also malicious. This signi cantly increases the number of discovered 
malicious domains. 
All proposed algorithms and representations are veri ed using extensive data sets spanning 
hundreds of independent networks. The validity of proposed approaches was further veri ed 
in a large-scale deployment within the Cisco Cognitive Threat Analytics.},
	urldate = {2019-04-02},
	school = {Czech Technical University in Prague},
	author = {Jusko, Ján},
	year = {2017},
	file = {Snapshot:/home/cisco/Zotero/storage/AZ5L27HZ/73702.html:text/html;Full Text PDF:/home/cisco/Zotero/storage/ZE753DCR/Jusko - 2017 - Graph-based Detection of Malicious Network Communi.pdf:application/pdf}
}

@article{kohout_network_2018,
	title = {Network {Traffic} {Fingerprinting} {Based} on {Approximated} {Kernel} {Two}-{Sample} {Test}},
	volume = {13},
	issn = {1556-6013},
	doi = {10.1109/TIFS.2017.2768018},
	abstract = {Many applications and communication protocols exhibit unique communication patterns that can be exploited to identify them in network traffic. This paper proposes a method to represent these patterns compactly, such that they can be used in different analytical tasks. The method treats each communication as a set of observations of a random variable with unknown probability distribution. This view allows us to derive the representation from a distance between two probability distributions used in maximum mean discrepancy-a non-parametric kernel test. The representation (and distance) can be then easily used in various algorithms for identification of communicating application and data analysis, independently of the specific type of input data.},
	number = {3},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Kohout, J. and Pevný, T.},
	month = mar,
	year = {2018},
	keywords = {analytical tasks, application identification, approximated kernel two-sample test, Communication fingerprinting, data analysis, Inspection, Kernel, Machine learning algorithms, maximum mean discrepancy, Memory management, network traffic fingerprinting, nonparametric kernel test, nonparametric statistics, Probability distribution, probability distributions, protocols, Protocols, Random variables, statistical distributions, telecommunication traffic, unique communication patterns},
	pages = {788--801},
	file = {IEEE Xplore Abstract Record:/home/cisco/Zotero/storage/JGIGN2GK/8089373.html:text/html}
}

@article{innes_flux:_2018,
	title = {Flux: {Elegant} machine learning with {Julia}},
	shorttitle = {Flux},
	doi = {10.21105/joss.00602},
	journal = {Journal of Open Source Software},
	author = {Innes, Mike},
	year = {2018}
}

@article{bezanson_julia:_2017,
	title = {Julia: {A} {Fresh} {Approach} to {Numerical} {Computing}},
	volume = {59},
	issn = {0036-1445},
	shorttitle = {Julia},
	url = {https://epubs.siam.org/doi/10.1137/141000671},
	doi = {10.1137/141000671},
	abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical  computing. Julia is  designed to be easy and fast and questions notions generally held to be “laws of nature"  by practitioners of numerical computing: {\textbackslash}beginlist {\textbackslash}item  High-level dynamic programs have to be slow. {\textbackslash}item  One must prototype in one language and then rewrite in another language for speed or deployment. {\textbackslash}item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. {\textbackslash}endlist We introduce the  Julia programming language and its design---a  dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch,  a  technique from computer science, picks  the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that  one can achieve machine performance without sacrificing human convenience.},
	number = {1},
	urldate = {2018-09-01},
	journal = {SIAM Review},
	author = {Bezanson, J. and Edelman, A. and Karpinski, S. and Shah, V.},
	month = jan,
	year = {2017},
	pages = {65--98}
}

@inproceedings{glorot_understanding_2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	url = {http://proceedings.mlr.press/v9/glorot10a.html},
	abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental resul...},
	urldate = {2018-09-01},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Glorot, Xavier and Bengio, Yoshua},
	month = mar,
	year = {2010},
	pages = {249--256}
}

@article{hahnloser_digital_2000,
	title = {Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit},
	volume = {405},
	copyright = {2000 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/35016072},
	doi = {10.1038/35016072},
	abstract = {Digital circuits such as the flip-flop use feedback to achieve multi-stability and nonlinearity to restore signals to logical levels, for example 0 and 1. Analogue feedback circuits are generally designed to operate linearly, so that signals are over a range, and the response is unique. By contrast, the response of cortical circuits to sensory stimulation can be both multistable and graded1,2,3,4. We propose that the neocortex combines digital selection of an active set of neurons with analogue response by dynamically varying the positive feedback inherent in its recurrent connections. Strong positive feedback causes differential instabilities that drive the selection of a set of active neurons under the constraints embedded in the synaptic weights. Once selected, the active neurons generate weaker, stable feedback that provides analogue amplification of the input. Here we present our model of cortical processing as an electronic circuit that emulates this hybrid operation, and so is able to perform computations that are similar to stimulus selection, gain modulation and spatiotemporal pattern generation in the neocortex.},
	number = {6789},
	urldate = {2018-08-27},
	journal = {Nature},
	author = {Hahnloser, Richard H. R. and Sarpeshkar, Rahul and Mahowald, Misha A. and Douglas, Rodney J. and Seung, H. Sebastian},
	month = jun,
	year = {2000},
	pages = {947--951}
}

@inproceedings{pevny_using_2017,
	title = {Using {Neural} {Network} {Formalism} to {Solve} {Multiple}-{Instance} {Problems}},
	isbn = {978-3-319-59072-1},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-59072-1_17},
	doi = {10.1007/978-3-319-59072-1_17},
	abstract = {Many objects in the real world are difficult to describe by means of a single numerical vector of a fixed length, whereas describing them by means of a set of vectors is more natural. Therefore, Multiple instance learning (MIL) techniques have been constantly gaining in importance throughout the last years. MIL formalism assumes that each object (sample) is represented by a set (bag) of feature vectors (instances) of fixed length, where knowledge about objects (e.g., class label) is available on bag level but not necessarily on instance level. Many standard tools including supervised classifiers have been already adapted to MIL setting since the problem got formalized in the late nineties. In this work we propose a neural network (NN) based formalism that intuitively bridges the gap between MIL problem definition and the vast existing knowledge-base of standard models and classifiers. We show that the proposed NN formalism is effectively optimizable by a back-propagation algorithm and can reveal unknown patterns inside bags. Comparison to 14 types of classifiers from the prior art on a set of 20 publicly available benchmark datasets confirms the advantages and accuracy of the proposed solution.},
	urldate = {2018-05-23},
	booktitle = {Advances in {Neural} {Networks} - {ISNN} 2017},
	publisher = {Springer, Cham},
	author = {Pevný, Tomáš and Somol, Petr},
	month = jun,
	year = {2017},
	pages = {135--142}
}

@inproceedings{chen_contextual_2012,
	address = {Sichuan, China},
	title = {Contextual {Hausdorff} dissimilarity for multi-instance clustering},
	isbn = {978-1-4673-0024-7},
	url = {https://ieeexplore.ieee.org/abstract/document/6233889/},
	doi = {10.1109/FSKD.2012.6233889},
	urldate = {2018-05-14},
	booktitle = {Fuzzy {Systems} and {Knowledge} {Discovery} ({FSKD})},
	publisher = {IEEE Computer Society Press},
	author = {Chen, Ying and Wu, Ou},
	month = may,
	year = {2012},
	pages = {870--873}
}

@article{chen_miles:_2006,
	title = {{MILES}: {Multiple}-{Instance} {Learning} via {Embedded} {Instance} {Selection}},
	volume = {28},
	issn = {0162-8828},
	shorttitle = {{MILES}},
	doi = {10.1109/TPAMI.2006.248},
	abstract = {Multiple-instance problems arise from the situations where training class labels are attached to sets of samples (named bags), instead of individual samples within each bag (called instances). Most previous multiple-instance learning (MIL) algorithms are developed based on the assumption that a bag is positive if and only if at least one of its instances is positive. Although the assumption works well in a drug activity prediction problem, it is rather restrictive for other applications, especially those in the computer vision area. We propose a learning method, MILES (multiple-instance learning via embedded instance selection), which converts the multiple-instance learning problem to a standard supervised learning problem that does not impose the assumption relating instance labels to bag labels. MILES maps each bag into a feature space defined by the instances in the training bags via an instance similarity measure. This feature mapping often provides a large number of redundant or irrelevant features. Hence, 1-norm SVM is applied to select important features as well as construct classifiers simultaneously. We have performed extensive experiments. In comparison with other methods, MILES demonstrates competitive classification accuracy, high computation efficiency, and robustness to labeling uncertainty},
	number = {12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Chen, Yixin and Bi, Jinbo and Wang, J. Z.},
	month = dec,
	year = {2006},
	keywords = {computer vision, object recognition, 1-norm support vector machine, Algorithms, Application software, Artificial Intelligence, classification accuracy, drug activity prediction, drug activity prediction., Drugs, embedded instance selection, feature extraction, feature mapping, feature subset selection, image categorization, Image Enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Information Storage and Retrieval, Labeling, labeling uncertainty, learning (artificial intelligence), Learning systems, MILES, Multiple-instance learning, multiple-instance learning algorithms, pattern classification, Reproducibility of Results, Robustness, Sensitivity and Specificity, supervised learning, support vector machine, Support vector machine classification, support vector machines, Uncertainty},
	pages = {1931--1947}
}

@inproceedings{wang_solving_2000,
	address = {Stanford University, Stanford, CA, USA},
	title = {Solving {Multiple}-{Instance} {Problem}: {A} {Lazy} {Learning} {Approach}},
	shorttitle = {Solving {Multiple}-{Instance} {Problem}},
	url = {http://cogprints.org/2124/},
	abstract = {As opposed to traditional supervised learning, multiple-instance learning 
    concerns the problem of classifying a bag of instances, given bags that are 
    labeled by a teacher as being overall positive or negative. Current research 
    mainly concentrates on adapting traditional concept learning to solve this 
    problem. In this paper we investigate the use of lazy learning and Hausdorff 
    distance to approach the multiple-instance problem. We present two variants of 
    the K-nearest neighbor algorithm, called Bayesian-KNN and Citation-KNN, solving 
    the multiple-instance problem. Experiments on the Drug discovery benchmark data 
    show that both algorithms are competitive with the best ones conceived in the 
    concept learning framework. Further work includes exploring of a combination of 
    lazy and eager multiple-instance problem classifiers.},
	urldate = {2017-07-01},
	booktitle = {Proceedings of the {Seventeenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann},
	author = {Wang, Jun and Zucker, Jean-Daniel},
	editor = {Langley, Pat},
	year = {2000},
	pages = {1119--1125}
}

@article{rippel_metric_2016,
	title = {Metric {Learning} with {Adaptive} {Density} {Discrimination}},
	url = {http://arxiv.org/abs/1511.05939},
	abstract = {Distance metric learning (DML) approaches learn a transformation to a representation space where distance is in correspondence with a predefined notion of similarity. While such models offer a number of compelling benefits, it has been difficult for these to compete with modern classification algorithms in performance and even in feature extraction. In this work, we propose a novel approach explicitly designed to address a number of subtle yet important issues which have stymied earlier DML algorithms. It maintains an explicit model of the distributions of the different classes in representation space. It then employs this knowledge to adaptively assess similarity, and achieve local discrimination by penalizing class distribution overlap. We demonstrate the effectiveness of this idea on several tasks. Our approach achieves state-of-the-art classification results on a number of fine-grained visual recognition datasets, surpassing the standard softmax classifier and outperforming triplet loss by a relative margin of 30-40\%. In terms of computational performance, it alleviates training inefficiencies in the traditional triplet loss, reaching the same error in 5-30 times fewer iterations. Beyond classification, we further validate the saliency of the learnt representations via their attribute concentration and hierarchy recovery properties, achieving 10-25\% relative gains on the softmax classifier and 25-50\% on triplet loss in these tasks.},
	urldate = {2020-06-05},
	journal = {arXiv:1511.05939 [cs, stat]},
	author = {Rippel, Oren and Paluri, Manohar and Dollar, Piotr and Bourdev, Lubomir},
	month = mar,
	year = {2016},
	note = {arXiv: 1511.05939},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning}
}

@article{friedman_use_1937,
	title = {The {Use} of {Ranks} to {Avoid} the {Assumption} of {Normality} {Implicit} in the {Analysis} of {Variance}},
	volume = {32},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1937.10503522},
	doi = {10.1080/01621459.1937.10503522},
	number = {200},
	journal = {Journal of the American Statistical Association},
	author = {Friedman, Milton},
	month = dec,
	year = {1937},
	note = {Publisher: Taylor \& Francis},
	pages = {675--701}
}

@article{nemenyi_distribution-free_1963,
	title = {Distribution-free multiple comparisons (doctoral dissertation, princeton university, 1963)},
	volume = {25},
	number = {2},
	journal = {Dissertation Abstracts International},
	author = {Nemenyi, PB},
	year = {1963},
	pages = {1233}
}

@article{kingma_adam:_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2020-06-25},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Learning, Computer Science - Machine Learning}
}

@article{pevny_nested_2020,
	title = {Nested {Multiple} {Instance} {Learning} in {Modelling} of {HTTP} network traffic},
	url = {http://arxiv.org/abs/2002.04059},
	abstract = {In many interesting cases, the application of machine learning is hindered by data having a complicated structure stimulated by a structured file-formats like JSONs, XMLs, or ProtoBuffers, which is non-trivial to convert to a vector / matrix. Moreover, since the structure frequently carries a semantic meaning, reflecting it in the machine learning model should improve the accuracy but more importantly it facilitates the explanation of decisions and the model. This paper demonstrates on the identification of infected computers in the computer network from their HTTP traffic, how to achieve this reflection using recent progress in multiple-instance learning. The proposed model is compared to complementary approaches from the prior art, the first relying on human-designed features and the second on automatically learned features through convolution neural networks. In a challenging scenario measuring accuracy only on unseen domains/malware families, the proposed model is superior to the prior art while providing a valuable feedback to the security researchers. We believe that the proposed framework will found applications elsewhere even beyond the field of security.},
	urldate = {2020-06-25},
	journal = {arXiv:2002.04059 [cs]},
	author = {Pevny, Tomas and Dedic, Marek},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.04059},
	keywords = {Computer Science - Cryptography and Security}
}

@inproceedings{weinberger_distance_2006,
	title = {Distance {Metric} {Learning} for {Large} {Margin} {Nearest} {Neighbor} {Classification}},
	url = {http://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 18},
	publisher = {MIT Press},
	author = {Weinberger, Kilian Q and Blitzer, John and Saul, Lawrence K.},
	editor = {Weiss, Y. and Schölkopf, B. and Platt, J. C.},
	year = {2006},
	pages = {1473--1480}
}

@mastersthesis{dedic_optimalization_2020,
	address = {Prague},
	title = {Optimalization of distances for multi-instance clustering},
	copyright = {All rights reserved},
	abstract = {Clustering is a prime example of a problem typically associated with unsupervised learning. One of the key design choices when using any clustering algorithm is to choose the right distance metric. In this work, an approach for using machine learning to learn the metric is introduced. The approach build on multi-instance learning, an approach offering high computational performance and a strong expressive power for describing data with an inherent structure using hierarchical models. Three methods building on multi-instance learning are presented together with the prior art they build upon. One of the methods is unsupervised while two are supervised. The methods are theoretically discussed and experimentally evaluated on publicly available datasets for multi-instance learning, as well as a corporate dataset of network security data. The results are then discussed and the methods compared.},
	school = {Czech Technical University in Prague},
	author = {Dědič, Marek},
	month = jan,
	year = {2020}
}

@article{zhang_multi-instance_2009,
	title = {Multi-instance clustering with applications to multi-instance prediction},
	volume = {31},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-007-0111-x},
	doi = {10.1007/s10489-007-0111-x},
	abstract = {In the setting of multi-instance learning, each object is represented by a bag composed of multiple instances instead of by a single instance in a traditional learning setting. Previous works in this area only concern multi-instance prediction problems where each bag is associated with a binary (classification) or real-valued (regression) label. However, unsupervised multi-instance learning where bags are without labels has not been studied. In this paper, the problem of unsupervised multi-instance learning is addressed where a multi-instance clustering algorithm named Bamic is proposed. Briefly, by regarding bags as atomic data items and using some form of distance metric to measure distances between bags, Bamic adapts the popular k-Medoids algorithm to partition the unlabeled training bags into k disjoint groups of bags. Furthermore, based on the clustering results, a novel multi-instance prediction algorithm named Bartmip is developed. Firstly, each bag is re-represented by a k-dimensional feature vector, where the value of the i-th feature is set to be the distance between the bag and the medoid of the i-th group. After that, bags are transformed into feature vectors so that common supervised learners are used to learn from the transformed feature vectors each associated with the original bag’s label. Extensive experiments show that Bamic could effectively discover the underlying structure of the data set and Bartmip works quite well on various kinds of multi-instance prediction problems.},
	language = {en},
	number = {1},
	urldate = {2020-07-30},
	journal = {Applied Intelligence},
	author = {Zhang, Min-Ling and Zhou, Zhi-Hua},
	month = aug,
	year = {2009},
	pages = {47--68}
}
